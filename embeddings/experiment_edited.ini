[vars]
model_dir="mmmt-en-de"
data_dir="/a/merkur3/cifka/diplomka/experiments/mmmt-ende/data"

[main]
name="EN -> DE translation"
tf_manager=<tf_manager>
output=$model_dir
batch_size=32
epochs=20
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<emb_runner>]
postprocess=None
evaluation=[("target", <bleu>)]
visualize_embeddings=[<input_sequence>]
logging_period=200
validation_period=1000
runners_batch_size=32
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1

[german_preprocessor]
class=processors.german.GermanPreprocessor

[german_postprocessor]
class=processors.german.GermanPostprocessor

[bleu]
class=evaluators.bleu.BLEUEvaluator

[train_data]
class=dataset.from_files
s_source="{data_dir}/t1_ext/train.t1_ext.en.unesc"
s_target="{data_dir}/t1_ext/train.t1_ext.de.unesc"
preprocessors=[("target", "target_prep", <german_preprocessor>)]

[val_data]
class=dataset.from_files
s_target="{data_dir}/task1/de/val/val.de.tok.tc.unesc"
s_source="{data_dir}/task1/en/val/val.en.tok.tc.unesc"
preprocessors=[("target", "target_prep", <german_preprocessor>)]

[encoder_vocabulary]
class=vocabulary.from_wordlist
path="{model_dir}/en.vocab.unesc"

[decoder_vocabulary]
class=vocabulary.from_wordlist
path="{model_dir}/de.vocab.unesc"

[input_sequence]
class=model.sequence.EmbeddedSequence
name="input"
embedding_size=300
max_length=30
data_id="source"
vocabulary=<encoder_vocabulary>

[encoder]
class=encoders.recurrent.RecurrentEncoder
name="encoder"
input_sequence=<input_sequence>
rnn_size=300
dropout_keep_prob=0.9
rnn_cell="NematusGRU"

[encoder_context]
class=attention.stateful_context.StatefulContext
name="encoder_context"
encoder=<encoder>

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<encoder>]
attentions=[<encoder_context>]
conditional_gru=True
vocabulary=<decoder_vocabulary>
data_id="target_prep"
max_output_len=35
dropout_keep_prob=0.6
embedding_size=300
rnn_cell="NematusGRU"
rnn_size=600

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
l2_weight=1.0e-8
clip_norm=1.0

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=<german_postprocessor>

[emb_runner]
class=runners.tensor_runner.RepresentationRunner
output_series="encoded"
encoder=<encoder>
